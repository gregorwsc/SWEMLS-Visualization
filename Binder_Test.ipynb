{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "intro_text = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6; padding: 10px; border: 1px solid #ccc; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "        <h2 style=\"margin-top: 0;\"> SWEMLS Visualizer</h2>\n",
    "        <p>This tool enables the visualization of metadata and workflows of hybrid AI systems described in the Semantic Web and Machine Leanring Systems Ontology (Ekaputra et al. 2023) and generates draw.io-compatible XML files.</p>\n",
    "        <p>üîó You can find more information about the SWEMLS Ontology here:<br>\n",
    "            <a href=\"https://semsys.ai.wu.ac.at/sites/swemls-kg/\" target=\"_blank\">\n",
    "                https://semsys.ai.wu.ac.at/sites/swemls-kg/\n",
    "            </a>\n",
    "        </p>\n",
    "\n",
    "        <h4 style=\"margin-top: 20px;\"> User Guide</h4>\n",
    "        <ul style=\"padding-left: 20px;\">\n",
    "            <li><strong> Select your data source:</strong><br>\n",
    "                Use the provided example file (containing 400+ systems) or upload your own TTL file.<br>\n",
    "                Click <em>Continue</em> to proceed.\n",
    "            </li>\n",
    "            <li><strong> Search for a system:</strong><br>\n",
    "                Use a SPARQL query to find a matching system.<br>\n",
    "                Confirm your selection by clicking <em>Confirm Selection</em>.<br>\n",
    "                The default query searches for systems that match specific patterns (e.g., A1).\n",
    "            </li>\n",
    "            <li><strong> Choose a visualization method:</strong><br>\n",
    "                If the pattern is known, choose one of the following options:\n",
    "                <ul style=\"padding-left: 20px;\">\n",
    "                    <li> Use a manually created template</li>\n",
    "                    <li> Automatically generate the workflow using the layout algorithm</li>\n",
    "                </ul>\n",
    "                Click <em>Continue with Selected Option</em> to proceed.<br>\n",
    "                To reset your selection, simply click <em>Confirm Selection</em> (selecting the same system) again.\n",
    "            </li>\n",
    "            <li><strong> If the pattern is unknown:</strong><br>\n",
    "                Please upload a valid pattern TTL file and click <em>Continue with Uploaded Pattern</em>.\n",
    "            </li>\n",
    "            <li><strong> Download:</strong><br>\n",
    "                Click the generated download link to obtain the XML file.<br>\n",
    "                You can import this file into draw.io to view the visualization.\n",
    "            </li>\n",
    "        </ul>\n",
    "        <hr style=\"margin-top: 20px;\">\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    layout=widgets.Layout(margin='0 0 20px 0')\n",
    ")\n",
    "\n",
    "\n",
    "display(intro_text)\n",
    "\n",
    "\n",
    "# Definiere das Output-Widget\n",
    "error_output = widgets.Output()\n",
    "display(error_output)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "METADATA_TEMPLATE_PATH = \"Templates/MetaTemp.xml\"\n",
    "MERGED_OUTPUT_PATH = \"Merged_Workflow_Output.xml\"\n",
    "\n",
    "def finalize_and_merge_xml(xml_string_or_path):\n",
    "    with download_output:\n",
    "        download_output.clear_output()\n",
    "        try:\n",
    "            with open(METADATA_TEMPLATE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta_xml_template = f.read()\n",
    "\n",
    "            updated_meta_xml = replace_paper_placeholders(meta_xml_template, instance_data)\n",
    "            meta_output_path = \"Updated_Metadata.xml\"\n",
    "\n",
    "            with open(meta_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(updated_meta_xml)\n",
    "\n",
    "            if isinstance(xml_string_or_path, str) and not os.path.isfile(xml_string_or_path):\n",
    "                workflow_xml_path = \"Generated_Workflow.drawio\"\n",
    "                with open(workflow_xml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(xml_string_or_path)\n",
    "            else:\n",
    "                workflow_xml_path = xml_string_or_path\n",
    "\n",
    "            # üõ°Ô∏è Pr√ºfung vor Merge\n",
    "            #print(f\"üîç Merge-Vorbereitung:\")\n",
    "            #print(f\"  ‚Üí Metadata XML: {meta_output_path}, Gr√∂√üe: {os.path.getsize(meta_output_path)} Bytes\")\n",
    "            #print(f\"  ‚Üí Workflow XML: {workflow_xml_path}, Gr√∂√üe: {os.path.getsize(workflow_xml_path)} Bytes\")\n",
    "\n",
    "            with open(workflow_xml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                first_line = f.readline()\n",
    "                #print(f\"  ‚Üí Erste Zeile der Workflow-XML: {first_line.strip()}\")\n",
    "\n",
    "            if os.path.getsize(workflow_xml_path) == 0:\n",
    "                #print(f\"‚ùå Merge abgebrochen: Workflow-XML '{workflow_xml_path}' ist leer.\")\n",
    "                return\n",
    "\n",
    "            merge_drawio_xml(meta_output_path, workflow_xml_path, MERGED_OUTPUT_PATH)\n",
    "\n",
    "            b64 = base64.b64encode(open(MERGED_OUTPUT_PATH, \"rb\").read()).decode()\n",
    "            href = f'data:application/xml;base64,{b64}'\n",
    "            display(HTML(f'<a download=\"{MERGED_OUTPUT_PATH}\" href=\"{href}\" target=\"_blank\">‚¨áÔ∏è Click here to download the XML</a>'))\n",
    "\n",
    "        except Exception as e:\n",
    "            show_exception(e)\n",
    "\n",
    "\n",
    "\n",
    "def style_component_cells(cells, stroke_width=2.5):\n",
    "    #print(\"[INFO] Setze dicke Rahmen f√ºr Komponenten mit shape=rectangle oder shape=hexagon\")\n",
    "    for cell in cells:\n",
    "        style = cell.attrib.get(\"style\", \"\")\n",
    "        if \"shape=rectangle\" in style or \"shape=hexagon\" in style:\n",
    "            # strokeWidth setzen oder ersetzen\n",
    "            if \"strokeWidth=\" in style:\n",
    "                style = re.sub(r'strokeWidth=\\d+(\\.\\d+)?', f'strokeWidth={stroke_width}', style)\n",
    "            else:\n",
    "                if not style.endswith(\";\") and style != \"\":\n",
    "                    style += \";\"\n",
    "                style += f\"strokeWidth={stroke_width};\"\n",
    "\n",
    "            # strokeColor NICHT setzen!\n",
    "            cell.attrib[\"style\"] = style\n",
    "\n",
    "\n",
    "def load_root_cells(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    diagram_root = root.find('.//root')\n",
    "    cells = diagram_root.findall('mxCell')\n",
    "    return tree, root, cells\n",
    "\n",
    "def get_min_y(cells):\n",
    "    return min(\n",
    "        (float(cell.find(\"mxGeometry\").attrib.get(\"y\", \"0\"))\n",
    "         for cell in cells if cell.find(\"mxGeometry\") is not None),\n",
    "        default=0.0\n",
    "    )\n",
    "\n",
    "def get_min_x(cells):\n",
    "    return min(\n",
    "        (float(cell.find(\"mxGeometry\").attrib.get(\"x\", \"0\"))\n",
    "         for cell in cells if cell.find(\"mxGeometry\") is not None),\n",
    "        default=0.0\n",
    "    )\n",
    "\n",
    "def get_left_x_by_cell_id(cells, target_id):\n",
    "    for cell in cells:\n",
    "        if cell.get(\"id\") == target_id:\n",
    "            geom = cell.find(\"mxGeometry\")\n",
    "            if geom is not None:\n",
    "                return float(geom.attrib.get(\"x\", \"0\"))\n",
    "    return 0.0\n",
    "\n",
    "def apply_y_offset(cells, offset_y=0):\n",
    "    #print(f\"[INFO] Verschiebe Workflow vertikal um y = {offset_y}\")\n",
    "    for cell in cells:\n",
    "        geom = cell.find(\"mxGeometry\")\n",
    "        if geom is not None:\n",
    "            y = float(geom.attrib.get(\"y\", \"0\"))\n",
    "            geom.attrib[\"y\"] = str(y + offset_y)\n",
    "\n",
    "def apply_x_offset(cells, offset_x=0):\n",
    "    #print(f\"[INFO] Verschiebe Workflow horizontal um x = {offset_x}\")\n",
    "    for cell in cells:\n",
    "        geom = cell.find(\"mxGeometry\")\n",
    "        if geom is not None:\n",
    "            x = float(geom.attrib.get(\"x\", \"0\"))\n",
    "            geom.attrib[\"x\"] = str(x + offset_x)\n",
    "\n",
    "def shift_workflow_nodes_below_metadata(root, table_ids, padding=50):\n",
    "    max_y = 0\n",
    "    for table_id in table_ids:\n",
    "        table_cell = root.find(f\".//mxCell[@id='{table_id}']\")\n",
    "        if table_cell is not None:\n",
    "            geom = table_cell.find(\"mxGeometry\")\n",
    "            if geom is not None:\n",
    "                try:\n",
    "                    y = float(geom.get(\"y\", \"0\"))\n",
    "                    h = float(geom.get(\"height\", \"0\"))\n",
    "                    max_y = max(max_y, y + h)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return max_y + padding\n",
    "\n",
    "def merge_drawio_xml(xml1_path, xml2_path, output_path):\n",
    "    tree1, root1, cells1 = load_root_cells(xml1_path)\n",
    "    _, root2, cells2 = load_root_cells(xml2_path)\n",
    "\n",
    "    table_ids = [\n",
    "        \"dXy3HHlULE7lc8IMxovX-26\", #sys\n",
    "        \"dXy3HHlULE7lc8IMxovX-1\",  #paper\n",
    "        \"dXy3HHlULE7lc8IMxovX-48\"   # doc\n",
    "    ]\n",
    "\n",
    "    workflow_cells = [\n",
    "        c for c in cells2\n",
    "        if c.get(\"vertex\") == \"1\" and c.get(\"parent\") not in table_ids\n",
    "    ]\n",
    "\n",
    "    #print(\"=== DEBUG: Vorher - Workflow-Knotenpositionen ===\")\n",
    "    for cell in workflow_cells:\n",
    "        geom = cell.find(\"mxGeometry\")\n",
    "        #if geom is not None:\n",
    "            #print(f\"  ‚Ü≥ ID: {cell.get('id')}, x: {geom.get('x', '?')}, y: {geom.get('y', '?')}\")\n",
    "\n",
    "    offset_y = shift_workflow_nodes_below_metadata(root1, table_ids, padding=50)\n",
    "    min_workflow_y = get_min_y(workflow_cells)\n",
    "    apply_y_offset(workflow_cells, offset_y - min_workflow_y)\n",
    "\n",
    "    system_x = get_left_x_by_cell_id(cells1, \"dXy3HHlULE7lc8IMxovX-26\")\n",
    "    min_x = get_min_x(workflow_cells)\n",
    "    apply_x_offset(workflow_cells, system_x - min_x)\n",
    "    style_component_cells(workflow_cells, stroke_width=2.5)\n",
    "\n",
    "    #print(\"=== DEBUG: Nachher - Workflow-Knotenpositionen ===\")\n",
    "    for cell in workflow_cells:\n",
    "        geom = cell.find(\"mxGeometry\")\n",
    "        #if geom is not None:\n",
    "           # print(f\"  ‚Ü≥ ID: {cell.get('id')}, x: {geom.get('x', '?')}, y: {geom.get('y', '?')}\")\n",
    "\n",
    "    # Merging beide root-Bl√∂cke\n",
    "    merged_root = ET.Element(\"root\")\n",
    "    seen_ids = set()\n",
    "    base_ids = {\"0\", \"1\"}\n",
    "\n",
    "    for cell in cells1 + cells2:\n",
    "        cid = cell.get(\"id\")\n",
    "        if cid in base_ids:\n",
    "            if cid not in seen_ids:\n",
    "                merged_root.append(cell)\n",
    "                seen_ids.add(cid)\n",
    "        else:\n",
    "            merged_root.append(cell)\n",
    "\n",
    "    graph_model = tree1.find(\".//mxGraphModel\")\n",
    "    old_root = graph_model.find(\"root\")\n",
    "    graph_model.remove(old_root)\n",
    "    graph_model.append(merged_root)\n",
    "\n",
    "    tree1.write(output_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "    #print(f\"‚úÖ Merged XML saved to: {output_path}\")\n",
    "\n",
    "# ‚úÖ Finaler Check: node_aliases, all_nodes, label_map\n",
    "def replace_paper_placeholders(xml_string, instance_data):\n",
    "    tree = ET.ElementTree(ET.fromstring(xml_string))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    paper = instance_data[\"papers\"][0]\n",
    "    paper_meta = paper[\"metadata\"]\n",
    "    system = instance_data\n",
    "\n",
    "    # Ersetzungen vorbereiten\n",
    "    replacements = {\n",
    "        \"Add.Title\": paper_meta.get(\"has_title\", [\"\"])[0],\n",
    "        \"Add.AuthorCountry\": paper_meta.get(\"hasAuthorsCountry\", [\"\"])[0].split(\"#\")[-1],\n",
    "        \"Add.Year\": paper_meta.get(\"year\", [\"\"])[0],\n",
    "        \"Add.Venue\": paper_meta.get(\"venue\", [\"\"])[0],\n",
    "        \"Add.Keyword\": \", \".join(paper_meta.get(\"hasKeyword\", [])),\n",
    "        \"Add.ID\": paper_meta.get(\"has_ID\", [\"\"])[0],\n",
    "        \"Add.PaperType\": paper_meta.get(\"isOfType\", [\"\"])[0].split(\".\")[-1],\n",
    "        \"Add.Reports\": paper_meta.get(\"reports\", [\"\"])[0].split(\"/\")[-1],\n",
    "        \"Add.AplicationDomain\": system[\"relationships\"].get(\"hasApplicationDomain\", [\"\"])[0].split(\".\")[-1],\n",
    "        \"Add.Area\": system[\"relationships\"].get(\"hasArea\", [\"\"])[0].split(\".\")[-1],\n",
    "        \"Add.Pattern\": system[\"relationships\"].get(\"hasCorrespondingPattern\", [\"\"])[0].split(\".\")[-1],\n",
    "        \"Add.StatisticalModels\": \", \".join(m.split(\".\")[-1] for m in system[\"relationships\"].get(\"hasStatisticalModel\", [])),\n",
    "        \"Add.SystemMaturity\": system[\"relationships\"].get(\"hasSystemMaturity\", [\"\"])[0].split(\".\")[-1],\n",
    "        \"Add.Task\": system[\"relationships\"].get(\"hasTask\", [\"\"])[0].split(\".\")[-1],\n",
    "        \"Add.TrainingType\": system[\"relationships\"].get(\"hasTrainingType\", [\"\"])[0].split(\".\")[-1],\n",
    "        \"Add.EvaluationData\": system[\"documentation\"].get(\"evaluation_data\", \"\"),\n",
    "        \"Add.EvaluationDataSplit\": system[\"documentation\"].get(\"evaluation_data_split\", \"\"),\n",
    "        \"Add.EvaluationMetrics\": system[\"documentation\"].get(\"evaluation_metrics\", \"\"),\n",
    "        \"Add.Infrastructure\": system[\"documentation\"].get(\"infrastructure\", \"\"),\n",
    "        \"Add.Parameters\": system[\"documentation\"].get(\"parameters\", \"\"),\n",
    "        \"Add.ProcessSteps\": system[\"documentation\"].get(\"process_steps\", \"\"),\n",
    "        \"Add.ProvenanceSupport\": system[\"documentation\"].get(\"provenance_support\", \"\"),\n",
    "        \"Add.Software\": system[\"documentation\"].get(\"software\", \"\"),\n",
    "        \"Add.SystemID\": system[\"id\"]\n",
    "    }\n",
    "\n",
    "    for element in root.iter(\"mxCell\"):\n",
    "        if 'value' in element.attrib:\n",
    "            value = element.attrib['value']\n",
    "            for placeholder, real_value in replacements.items():\n",
    "                if placeholder in value:\n",
    "                    replacement = real_value.replace(\"_\", \" \") if real_value and str(real_value).strip() else \"Missing\"\n",
    "                    element.attrib['value'] = value.replace(placeholder, replacement)\n",
    "                    \n",
    "                    \n",
    "                    # system √ºberschirft soll auch fett sein\n",
    "                    if placeholder == \"Add.SystemID\":\n",
    "                        current_style = element.attrib.get('style', '')\n",
    "                        if \"fontStyle=1\" not in current_style:\n",
    "                            if \"fontStyle=\" in current_style:\n",
    "                                current_style = re.sub(r'fontStyle=\\d+', 'fontStyle=1', current_style)\n",
    "                            else:\n",
    "                                if not current_style.endswith(';') and current_style != '':\n",
    "                                    current_style += ';'\n",
    "                                current_style += 'fontStyle=1;'\n",
    "                            element.attrib['style'] = current_style\n",
    "\n",
    "    return ET.tostring(root, encoding='utf8', method='xml').decode()\n",
    "\n",
    "\n",
    "def generate_drawio_xml(positions, components, edges, label_map=None):\n",
    "    #print(\"\\nüîπ FUNCTION CALLED: generate_drawio_xml\")\n",
    "\n",
    "    mxfile = ET.Element(\"mxfile\", host=\"Electron\", version=\"26.0.16\")\n",
    "    diagram = ET.SubElement(mxfile, \"diagram\", name=\"Generated Workflow\")\n",
    "    mxGraphModel = ET.SubElement(diagram, \"mxGraphModel\", dx=\"913\", dy=\"789\", grid=\"0\", gridSize=\"10\", \n",
    "                                 guides=\"1\", tooltips=\"1\", connect=\"1\", arrows=\"1\", fold=\"1\", page=\"1\", \n",
    "                                 pageScale=\"1\", pageWidth=\"827\", pageHeight=\"1169\", background=\"none\", \n",
    "                                 math=\"0\", shadow=\"0\")\n",
    "    root = ET.SubElement(mxGraphModel, \"root\")\n",
    "\n",
    "    # Draw.io root cells\n",
    "    ET.SubElement(root, \"mxCell\", id=\"0\")\n",
    "    ET.SubElement(root, \"mxCell\", id=\"1\", parent=\"0\")\n",
    "\n",
    "    styles = {\n",
    "        \"data\": \"shape=hexagon;fillColor=#CCCCCC;strokeColor=#999999;\",\n",
    "        \"symbolic_data\": \"shape=hexagon;fillColor=#B9E0A5;strokeColor=#009900;\",\n",
    "        \"ml_component\": \"shape=rectangle;fillColor=#00FFFF;strokeColor=#66B2FF;\",\n",
    "        \"sr_component\": \"shape=rectangle;fillColor=#00FF00;strokeColor=#008000;\",\n",
    "        \"kr_component\": \"shape=rectangle;fillColor=#00FF00;strokeColor=#008000;\",\n",
    "        \"unclassified\": \"shape=ellipse;fillColor=#FFA07A;strokeColor=#FF4500;\"\n",
    "    }\n",
    "\n",
    "    node_elements = {}\n",
    "\n",
    "    #print(\"\\nüìå DEBUG: Components received in function:\")\n",
    "    #for category, nodes in components.items():\n",
    "        #print(f\"  - {category}: {nodes}\")\n",
    "\n",
    "    for category, nodes in components.items():\n",
    "        for node in nodes:\n",
    "            style = styles.get(category, \"shape=ellipse;\")\n",
    "\n",
    "            # üÜï Neu: Wenn label_map vorhanden und Node bekannt ist, nutze das lesbare Label\n",
    "            if label_map and node in label_map:\n",
    "                cleaned_name = label_map[node]\n",
    "            else:\n",
    "                cleaned_name = node.replace(\"Custom.\", \"\").replace(\"Resource.\", \"\")\n",
    "\n",
    "            length = len(cleaned_name)\n",
    "            if length > 24:\n",
    "                style += \"fontSize=5\"\n",
    "            elif length > 20:\n",
    "                style += \"fontSize=7;\"\n",
    "            elif length > 17:\n",
    "                style += \"fontSize=9;\"\n",
    "\n",
    "            pos = positions.get(node, (100, 100))\n",
    "            mxCell = ET.SubElement(root, \"mxCell\", id=node, value=cleaned_name, style=style, vertex=\"1\", parent=\"1\")\n",
    "            mxGeometry = ET.SubElement(mxCell, \"mxGeometry\", x=str(pos[0]), y=str(pos[1]), width=\"120\", height=\"80\")\n",
    "            mxGeometry.set(\"as\", \"geometry\")\n",
    "\n",
    "            node_elements[node] = node\n",
    "\n",
    "    #print(\"\\nüìå DEBUG: Checking node elements before edge creation:\", node_elements.keys())\n",
    "    #print(\"\\nüìå DEBUG: Edges list inside function (before alias replacement):\", edges)\n",
    "\n",
    "    updated_edges = []\n",
    "    for source, target in edges:\n",
    "        new_source = node_aliases.get(source, source)\n",
    "        new_target = node_aliases.get(target, target)\n",
    "        updated_edges.append((new_source, new_target))\n",
    "\n",
    "    edges = updated_edges\n",
    "\n",
    "    #print(\"\\nüìå DEBUG: Reached edge processing loop...\")\n",
    "\n",
    "    for source, target in edges:\n",
    "        #print(f\"  üîç DEBUG: Processing edge {source} ‚Üí {target}\")\n",
    "        if source in node_elements and target in node_elements:\n",
    "            #print(f\"  ‚úÖ Creating edge: {source} ‚Üí {target}\")\n",
    "            edge_id = f\"edge_{source}_{target}\"\n",
    "\n",
    "            # üß† Improved orthogonal edge style for smooth top-down flow\n",
    "            style = (\n",
    "                \"edgeStyle=straight;\"\n",
    "                \"curved=0;\"\n",
    "                \"rounden=1;\"\n",
    "                \"html=1;\"\n",
    "                \"exitX=0.5;\"\n",
    "                \"exitY=1;\"\n",
    "                \"exitDx=0;\"\n",
    "                \"exitDy=0;\"\n",
    "                \"entryX=0.5;\"\n",
    "                \"entryY=0;\"\n",
    "                \"entryDx=0;\"\n",
    "                \"entryDy=0;\"\n",
    "            )\n",
    "\n",
    "            mxCell = ET.SubElement(root, \"mxCell\", \n",
    "                id=edge_id,\n",
    "                source=source,\n",
    "                target=target,\n",
    "                style=style,\n",
    "                edge=\"1\", parent=\"1\")\n",
    "            mxGeometry = ET.SubElement(mxCell, \"mxGeometry\", relative=\"1\")\n",
    "            mxGeometry.set(\"as\", \"geometry\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå DEBUG: Edge skipped (missing node): {source} ‚Üí {target}\")\n",
    "\n",
    "    return ET.tostring(mxfile, encoding='utf8', method='xml').decode()\n",
    "\n",
    "def finalize_layout_parameters():\n",
    "    global node_aliases, label_map\n",
    "\n",
    "    #print(\"\\nüß© Starte finalen Parameter-Check f√ºr Komponenten, Variablen und Aliase...\")\n",
    "\n",
    "    # ‚úÖ Dynamically create node_aliases from instance_data\n",
    "    node_aliases = {}\n",
    "\n",
    "    for step in instance_data[\"steps\"]:\n",
    "        short_id = step[\"id\"].split(\".\")[-1]\n",
    "        node_aliases[short_id] = step[\"id\"]\n",
    "        node_aliases[step[\"id\"]] = step[\"id\"]\n",
    "\n",
    "    for var_key, var_data in instance_data[\"variables\"].items():\n",
    "        node_aliases[var_key] = var_data[\"id\"]\n",
    "        node_aliases[var_data[\"id\"]] = var_data[\"id\"]\n",
    "\n",
    "    # Falls vorhanden, unclassified bereinigen\n",
    "    if \"unclassified\" in components:\n",
    "        components[\"unclassified\"] = [\n",
    "            node for node in components[\"unclassified\"]\n",
    "            if node not in node_aliases and node not in node_aliases.values()\n",
    "        ]\n",
    "        print(\"\\nüöÄ DEBUG: Cleaned 'unclassified' category to remove duplicates:\", components[\"unclassified\"])\n",
    "\n",
    "    # ‚úÖ Labels erstellen\n",
    "    label_map = {}\n",
    "    for step in instance_data[\"steps\"]:\n",
    "        full_id = step[\"id\"]\n",
    "        short_label = extract_label_short(step)\n",
    "        label_map[full_id] = short_label\n",
    "\n",
    "    #print(\"\\nüßæ Final Label Map:\")\n",
    "    #for node_id, label in label_map.items():\n",
    "        #print(f\"  {node_id} ‚Üí {label}\")\n",
    "\n",
    "    # ‚úÖ XML erzeugen\n",
    "    #print(\"\\nüß© Starte finale XML-Erzeugung mit generiertem Layout...\")\n",
    "    xml_output = generate_drawio_xml(positions, components, edges, label_map)\n",
    "\n",
    "    # ‚úÖ Download-Link erzeugen\n",
    "    instance_id = instance_data.get(\"id\", \"unknown\")\n",
    "    filename = f\"Generated_{instance_id}_Workflow.drawio\"\n",
    "    b64 = base64.b64encode(xml_output.encode()).decode()\n",
    "    href = f'data:application/xml;base64,{b64}'\n",
    "\n",
    "    #display(HTML(f'<a download=\"{filename}\" href=\"{href}\" target=\"_blank\">‚¨áÔ∏è Click here to download the generated XML</a>'))\n",
    "    #print(f\"‚úÖ File ready to Download: {filename}\")\n",
    "\n",
    "# Funktion zur Fehlerausgabe\n",
    "def show_exception(e):\n",
    "    with error_output:\n",
    "        error_output.clear_output()  # L√∂sche vorherige Ausgaben\n",
    "        print(\"‚ùå Exception caught:\")\n",
    "        traceback.print_exception(type(e), e, e.__traceback__, file=sys.stdout)\n",
    "\n",
    "def shift_with_cascade(pos_to_node, positions, x, y, shift_x):\n",
    "    \"\"\"\n",
    "    Versucht, (x, y) zu belegen ‚Äì wenn dort schon ein Node ist, wird er nach rechts verschoben,\n",
    "    inkl. Kaskade f√ºr weitere Blockaden.\n",
    "    \"\"\"\n",
    "    if (x, y) not in pos_to_node:\n",
    "        return x  # Kein Problem ‚Äì Platz ist frei\n",
    "\n",
    "    # Es gibt bereits einen Node hier ‚Üí verschiebe ihn nach rechts\n",
    "    blocked_node = pos_to_node[(x, y)]\n",
    "    new_x = shift_with_cascade(pos_to_node, positions, x + shift_x, y, shift_x)\n",
    "\n",
    "    #print(f\"  üîÅ Kollision: {blocked_node} ist auf ({x}, {y}) ‚Üí verschiebe nach ({new_x}, {y})\")\n",
    "    positions[blocked_node] = (new_x, y)\n",
    "    pos_to_node[(new_x, y)] = blocked_node\n",
    "    del pos_to_node[(x, y)]  # alte Position freigeben\n",
    "\n",
    "    return x  # Jetzt kann der urspr√ºngliche Node an diese Stelle\n",
    "\n",
    "def simple_overlap_correction(positions, edges, x_spacing=200, shift_x=200):\n",
    "    #print(\"\\nüß† Starte finalen √úberlappungs-Check mit Shift-Kaskade...\")\n",
    "\n",
    "    pos_to_node = {(x, y): node for node, (x, y) in positions.items()}\n",
    "    x_to_y = defaultdict(list)\n",
    "    for (x, y), node in pos_to_node.items():\n",
    "        x_to_y[x].append((y, node))\n",
    "\n",
    "    corrections = {}\n",
    "\n",
    "    for source, target in edges:\n",
    "        if source not in positions or target not in positions:\n",
    "            continue\n",
    "\n",
    "        x1, y1 = positions[source]\n",
    "        x2, y2 = positions[target]\n",
    "\n",
    "        if x1 == x2 and abs(y1 - y2) > 1:\n",
    "            y_min, y_max = sorted([y1, y2])\n",
    "            for y_mid, middle_node in x_to_y[x1]:\n",
    "                if y_min < y_mid < y_max:\n",
    "                    #print(f\"‚ö†Ô∏è √úberlappung erkannt bei Kante {source} ‚Üí {target} durch {middle_node} auf X={x1}, Y={y_mid}\")\n",
    "                    \n",
    "                    if target not in corrections:\n",
    "                        # Jetzt: Berechne neue X-Position mit Kaskade\n",
    "                        new_x = shift_with_cascade(pos_to_node, positions, x1 + shift_x, y2, shift_x)\n",
    "                        corrections[target] = (new_x, y2)\n",
    "                    break\n",
    "\n",
    "    for node, (new_x, new_y) in corrections.items():\n",
    "        #print(f\"‚û°Ô∏è Verschiebe Node {node} nach rechts auf X={new_x}\")\n",
    "        positions[node] = (new_x, new_y)\n",
    "        pos_to_node[(new_x, new_y)] = node\n",
    "\n",
    "    #if not corrections:\n",
    "     #   print(\"‚úÖ Keine √úberlappungen erkannt ‚Äì keine Verschiebungen n√∂tig.\")\n",
    "    #else:\n",
    "     #   print(\"‚úÖ √úberlappungs-Korrektur (mit Kaskade) abgeschlossen.\")\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def assign_coordinates(ordered_layers: Dict[int, List[str]], x_spacing: int = 200, y_spacing: int = 150) -> Dict[str, Tuple[int, int]]:\n",
    "    positions = {}\n",
    "    for rank in sorted(ordered_layers):\n",
    "        nodes = ordered_layers[rank]\n",
    "        for i, node in enumerate(nodes):\n",
    "            x = i * x_spacing\n",
    "            y = rank * y_spacing  \n",
    "            positions[node] = (x, y)\n",
    "    return positions\n",
    "\n",
    "# Funktion zur Berechnung von Kanten√ºberschneidungen\n",
    "def count_crossings(layers: Dict[int, List[str]], edges: List[Tuple[str, str]]) -> int:\n",
    "    try:\n",
    "        pos_in_layer = {node: (rank, i) for rank, nodes in layers.items() for i, node in enumerate(nodes)}\n",
    "        crossings = 0\n",
    "        for (u1, v1), (u2, v2) in itertools.combinations(edges, 2):\n",
    "            if u1 not in pos_in_layer or v1 not in pos_in_layer or u2 not in pos_in_layer or v2 not in pos_in_layer:\n",
    "                continue\n",
    "            r1_u, x1_u = pos_in_layer[u1]\n",
    "            r1_v, x1_v = pos_in_layer[v1]\n",
    "            r2_u, x2_u = pos_in_layer[u2]\n",
    "            r2_v, x2_v = pos_in_layer[v2]\n",
    "            if r1_v != r2_v:\n",
    "                continue  # Nur Kanten im gleichen Layer vergleichen\n",
    "            if (x1_u - x2_u) * (x1_v - x2_v) < 0:\n",
    "                crossings += 1\n",
    "        return crossings\n",
    "    except Exception as e:\n",
    "        show_exception(e)  # Fehlerausgabe aufrufen\n",
    "        return 0\n",
    "\n",
    "# Funktion zur gierigen Swap-Optimierung\n",
    "def greedy_swap_optimization(layers: Dict[int, List[str]], edges: List[Tuple[str, str]], iterations: int = 10) -> Dict[int, List[str]]:\n",
    "    try:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edges)\n",
    "        new_layers = {r: list(nodes) for r, nodes in layers.items()}  # Kopie der Schichten\n",
    "        #print(\"\\nüîÅ Starte Greedy-Swap Optimierung...\\n\")\n",
    "        for r in sorted(new_layers):\n",
    "            if r == 0:\n",
    "                continue  # Layer 0 nicht optimieren\n",
    "            improved = True\n",
    "            while improved:\n",
    "                improved = False\n",
    "                current = new_layers[r]\n",
    "                for i in range(len(current) - 1):\n",
    "                    swapped = list(current)\n",
    "                    swapped[i], swapped[i+1] = swapped[i+1], swapped[i]\n",
    "                    temp_layers = dict(new_layers)\n",
    "                    temp_layers[r] = swapped\n",
    "                    old_crossings = count_crossings(new_layers, edges)\n",
    "                    new_crossings = count_crossings(temp_layers, edges)\n",
    "                    if new_crossings < old_crossings:\n",
    "                        print(f\"  ‚úÖ Swap: {current[i]} ‚¨å {current[i+1]} reduziert Kreuzungen: {old_crossings} ‚Üí {new_crossings}\")\n",
    "                        new_layers[r] = swapped\n",
    "                        improved = True\n",
    "                        break\n",
    "        return new_layers\n",
    "    except Exception as e:\n",
    "        show_exception(e)  # Fehlerausgabe aufrufen\n",
    "        return {}\n",
    "\n",
    "# Funktion zur Anwendung der Median-Heuristik\n",
    "def apply_median_heuristic(rank_map: Dict[str, int], edges: List[Tuple[str, str]], sort_layer_0: bool = False) -> Dict[int, List[str]]:\n",
    "    try:\n",
    "        #print(\"\\nüîß Starte Median-Heuristik zur Anordnung der Knoten innerhalb von R√§ngen...\\n\")\n",
    "        \n",
    "        # Pr√ºfen, ob rank_map und edges korrekt sind\n",
    "        if not rank_map or not edges:\n",
    "            print(\"‚ùå Error: rank_map or edges are empty.\")\n",
    "            return {}\n",
    "        \n",
    "        layers = defaultdict(list)\n",
    "        for node, rank in rank_map.items():\n",
    "            layers[rank].append(node)\n",
    "\n",
    "        if not layers:\n",
    "            print(\"‚ùå Error: Layers are empty.\")\n",
    "            return {}\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edges)\n",
    "        ordered_layers = {}\n",
    "\n",
    "        for r in sorted(layers.keys()):\n",
    "            if r == 0 and not sort_layer_0:\n",
    "                ordered_layers[r] = layers[r]\n",
    "                continue\n",
    "            \n",
    "            def median(node):\n",
    "                preds = list(G.predecessors(node))\n",
    "                if not preds:\n",
    "                    return float('inf')\n",
    "                return sum([layers[rank_map[pred]].index(pred) for pred in preds if pred in layers[rank_map[pred]]]) / len(preds)\n",
    "\n",
    "            sorted_nodes = sorted(layers[r], key=median)\n",
    "            ordered_layers[r] = sorted_nodes\n",
    "            \n",
    "            # Ausgabe der vertauschten Komponenten\n",
    "            #print(f\"\\nRang {r} - urspr√ºngliche Reihenfolge: {layers[r]}\")\n",
    "            #print(f\"  Neu angeordnet: {sorted_nodes}\")\n",
    "\n",
    "        #print(\"üîß Ergebnis der Median-Heuristik:\")\n",
    "        #for rank in sorted(ordered_layers):\n",
    "            #print(f\"  Rang {rank}: {ordered_layers[rank]}\")\n",
    "        \n",
    "        return ordered_layers\n",
    "    except Exception as e:\n",
    "        show_exception(e)  # Fehlerausgabe aufrufen\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rdflib import Graph, Namespace\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n",
    "shacl_output = widgets.Output()\n",
    "\n",
    "# === SHACL-Regeln anwenden auf hochgeladene Datei ===\n",
    "def apply_shacl_rules(instance_file_path: str) -> Graph:\n",
    "    SH = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "    SWEMLS = Namespace(\"https://w3id.org/semsys/ns/swemls#\")\n",
    "    shapes_dir = \"Shapes\"  # relative Pfadangabe im Repo\n",
    "\n",
    "    shape_files = [\n",
    "        \"_generic-shapes.ttl\", \"A1-shapes.ttl\", \"A2-shapes.ttl\", \"A3-shapes.ttl\",\n",
    "        \"F1-shapes.ttl\", \"F2-shapes.ttl\", \"F3-shapes.ttl\", \"F4-shapes.ttl\",\n",
    "        \"I1-shapes.ttl\", \"I2-shapes.ttl\", \"I3-shapes.ttl\", \"I4-shapes.ttl\",\n",
    "        \"I5-shapes.ttl\", \"I6-shapes.ttl\", \"I7-shapes.ttl\", \"O1-shapes.ttl\",\n",
    "        \"O2-shapes.ttl\", \"O3-shapes.ttl\", \"O4-shapes.ttl\", \"T1-shapes.ttl\",\n",
    "        \"T2-shapes.ttl\", \"T3-shapes.ttl\", \"T4-shapes.ttl\", \"T5-shapes.ttl\",\n",
    "        \"T6-shapes.ttl\", \"T7-shapes.ttl\", \"T8-shapes.ttl\", \"T9-shapes.ttl\",\n",
    "        \"T10-shapes.ttl\", \"T11-shapes.ttl\", \"T12-shapes.ttl\", \"T13-shapes.ttl\",\n",
    "        \"T14-shapes.ttl\", \"T15-shapes.ttl\", \"T16-shapes.ttl\", \"T17-shapes.ttl\",\n",
    "        \"T18-shapes.ttl\", \"T19-shapes.ttl\", \"T20-shapes.ttl\", \"T21-shapes.ttl\",\n",
    "        \"T22-shapes.ttl\", \"T23-shapes.ttl\", \"Y1-shapes.ttl\", \"Y2-shapes.ttl\", \"Y4-shapes.ttl\"\n",
    "    ]\n",
    "\n",
    "    g_instance = Graph()\n",
    "    g_instance.parse(instance_file_path, format=\"turtle\")\n",
    "\n",
    "    triples_before = len(g_instance)\n",
    "\n",
    "    for shape_file in shape_files:\n",
    "        shape_path = os.path.join(shapes_dir, shape_file)\n",
    "        g_shape = Graph()\n",
    "\n",
    "        if not os.path.exists(shape_path):\n",
    "            with shacl_output:\n",
    "                print(f\"‚ö†Ô∏è Shape file not found: {shape_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            g_shape.parse(shape_path, format=\"turtle\")\n",
    "            with shacl_output:\n",
    "                print(f\"‚úÖ Loaded SHACL shape file: {shape_file}\")\n",
    "        except Exception as e:\n",
    "            with shacl_output:\n",
    "                print(f\"‚ùå Error loading {shape_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for rule in g_shape.subjects(predicate=SH.rule, object=None):\n",
    "            for _, _, construct_query in g_shape.triples((rule, SH.construct, None)):\n",
    "                query = str(construct_query)\n",
    "                try:\n",
    "                    g_instance.update(query)\n",
    "                except Exception as e:\n",
    "                    with shacl_output:\n",
    "                        print(f\"‚ùå Error executing rule from {shape_file}: {e}\")\n",
    "\n",
    "    triples_after = len(g_instance)\n",
    "    with shacl_output:\n",
    "        print(\"‚úÖ All rules applied.\")\n",
    "        print(f\"üìä Triples before: {triples_before}\")\n",
    "        print(f\"üìà Triples after: {triples_after}\")\n",
    "        print(f\"‚ûï Added: {triples_after - triples_before} triples\")\n",
    "\n",
    "    return g_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from rdflib import Graph\n",
    "\n",
    "# === Globale Variablen ===\n",
    "instance_file_path = None\n",
    "output = widgets.Output()\n",
    "result_graph = None\n",
    "\n",
    "\n",
    "\n",
    "# === Auswahlfeld: Beispiel oder Upload ===\n",
    "option_selector = widgets.ToggleButtons(\n",
    "    options=[(\"Use example file\", \"example\"), (\"Upload your own\", \"upload\")],\n",
    "    description=\"Select input:\",\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# === Upload-Widget (immer sichtbar) ===\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.ttl',\n",
    "    multiple=False,\n",
    "    description='Upload TTL file'\n",
    ")\n",
    "\n",
    "# === \"Continue\"-Button ===\n",
    "continue_button = widgets.Button(description=\"Continue\", button_style='primary')\n",
    "\n",
    "# === Auswahlhandler ===\n",
    "def on_option_change(change):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        if change['new'] == 'example':\n",
    "            global instance_file_path\n",
    "            instance_file_path = \"Instance_Files/swemls-instances.ttl\"\n",
    "            print(f\"üìÅ Example file selected:\\n‚Üí {instance_file_path}\")\n",
    "        elif change['new'] == 'upload':\n",
    "            print(\"üì§ Please upload a TTL file using the field below.\")\n",
    "\n",
    "option_selector.observe(on_option_change, names='value')\n",
    "\n",
    "# === Upload-Handler ===\n",
    "def on_upload(change):\n",
    "    global instance_file_path\n",
    "    if upload_widget.value:\n",
    "        uploaded = upload_widget.value[0]\n",
    "        file_name = uploaded['name']\n",
    "        instance_file_path = file_name\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(uploaded['content'])\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            print(f\"‚úÖ File uploaded and saved as:\\n‚Üí {file_name}\")\n",
    "\n",
    "upload_widget.observe(on_upload, names='value')\n",
    "\n",
    "# === Continue-Button-Handler ===\n",
    "def on_continue(b):\n",
    "    global result_graph\n",
    "    output.clear_output()\n",
    "\n",
    "    # üõë Spezialfall: Upload gew√§hlt, aber kein File hochgeladen\n",
    "    if option_selector.value == \"upload\" and not upload_widget.value:\n",
    "        with output:\n",
    "            print(\"‚ö†Ô∏è Please upload a TTL file before continuing.\")\n",
    "        return\n",
    "\n",
    "    # ‚ùì Sicherheitsabfrage: Ist ein Pfad gesetzt?\n",
    "    if not instance_file_path:\n",
    "        with output:\n",
    "            print(\"‚ö†Ô∏è No file selected or uploaded.\")\n",
    "        return\n",
    "\n",
    "    # ‚ùå Existiert die Datei √ºberhaupt?\n",
    "    if not os.path.exists(instance_file_path):\n",
    "        with output:\n",
    "            print(f\"‚ùå File not found on disk:\\n‚Üí {instance_file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if instance_file_path.endswith(\".ttl\") and not instance_file_path.startswith(\"Instance_Files/\"):\n",
    "            with output:\n",
    "                print(\"üîç SHACL rules will be applied to uploaded file.\")\n",
    "            result_graph = apply_shacl_rules(instance_file_path)\n",
    "        else:\n",
    "            g = Graph()\n",
    "            g.parse(instance_file_path, format=\"turtle\")\n",
    "            result_graph = g\n",
    "\n",
    "        with output:\n",
    "            print(f\"‚úÖ RDF file successfully loaded!\")\n",
    "            print(f\"üìÑ Triples in graph: {len(result_graph)}\")\n",
    "            print(f\"üîó Using file:\\n‚Üí {instance_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with output:\n",
    "            print(f\"‚ùå Error parsing TTL file:\\n‚Üí {e}\")\n",
    "\n",
    "    # ‚¨áÔ∏è SHACL-Ausgabe sichtbar machen\n",
    "    display(shacl_output)\n",
    "\n",
    "\n",
    "continue_button.on_click(on_continue)\n",
    "\n",
    "# === Anzeige aller Elemente ===\n",
    "display(option_selector, upload_widget, continue_button, output)\n",
    "\n",
    "# ‚¨ÖÔ∏è Wichtig: Initiale Auswahlverarbeitung starten\n",
    "on_option_change({'new': option_selector.value})\n",
    "\n",
    "# Query-Handler\n",
    "def on_query_run(b):\n",
    "    global matched_systems\n",
    "    query_interface_output.clear_output()\n",
    "    selection_output.clear_output()\n",
    "    system_selector.options = []\n",
    "\n",
    "    if 'result_graph' not in globals() or result_graph is None:\n",
    "        with query_interface_output:\n",
    "            print(\"‚ö†Ô∏è RDF graph not loaded.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        results = result_graph.query(query_input.value)\n",
    "    except Exception as e:\n",
    "        with query_interface_output:\n",
    "            print(f\"‚ùå Query error: {e}\")\n",
    "        return\n",
    "\n",
    "    matched_systems = []\n",
    "    for row in results:\n",
    "        uri = str(row.system)\n",
    "        sys_id = uri.split(\"/\")[-1]\n",
    "        matched_systems.append((sys_id, uri))\n",
    "\n",
    "    if not matched_systems:\n",
    "        with query_interface_output:\n",
    "            print(\"‚ö†Ô∏è No matching systems found.\")\n",
    "        return\n",
    "\n",
    "    system_selector.options = [(sys_id, uri) for sys_id, uri in matched_systems]\n",
    "\n",
    "    with query_interface_output:\n",
    "        print(f\"‚úÖ Found {len(matched_systems)} matching system(s):\")\n",
    "        for i, (sys_id, _) in enumerate(matched_systems):\n",
    "            print(f\" {i+1}: {sys_id}\")\n",
    "\n",
    "# Auswahl-Handler\n",
    "def on_confirm_selection(b):\n",
    "    global selected_system_id, selected_system_uri, instance_data\n",
    "    \n",
    "    pattern_structure = None\n",
    "\n",
    "    selected_label = system_selector.label\n",
    "    selected_uri = system_selector.value\n",
    "\n",
    "    if not selected_uri:\n",
    "        with selection_output:\n",
    "            selection_output.clear_output()\n",
    "            print(\"‚ö†Ô∏è No system selected.\")\n",
    "        return\n",
    "\n",
    "    selected_system_id = selected_label\n",
    "    selected_system_uri = selected_uri\n",
    "\n",
    "    with selection_output:\n",
    "        selection_output.clear_output()\n",
    "        print(f\"‚úÖ You selected: {selected_system_id}\")\n",
    "        print(f\"üîó URI: {selected_system_uri}\")\n",
    "        #print(f\"üîÑ Extracting instance data...\")\n",
    "\n",
    "    try:\n",
    "        # Extrahiere Instanz als JSON\n",
    "        extract_and_export_selected_system()\n",
    "\n",
    "        # Lade direkt die erzeugte JSON-Datei\n",
    "        json_file = f\"{selected_system_id}.json\"\n",
    "        load_instance_json(json_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        with selection_output:\n",
    "            print(\"‚ùå Error trying to load and extact instance data:\")\n",
    "            traceback.print_exception(type(e), e, e.__traceback__)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from rdflib.namespace import RDF\n",
    "from IPython.display import display\n",
    "\n",
    "# Outputs\n",
    "query_interface_output = widgets.Output()\n",
    "selection_output = widgets.Output()\n",
    "\n",
    "# SPARQL Query-Feld\n",
    "query_input = widgets.Textarea(\n",
    "    value=\"\"\"\n",
    "PREFIX swemls: <https://w3id.org/semsys/ns/swemls#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT DISTINCT ?system\n",
    "WHERE {\n",
    "  ?system a swemls:System .\n",
    "  ?system swemls:hasCorrespondingPattern ?pattern .\n",
    "  FILTER(STRENDS(STR(?pattern), \"A1\"))\n",
    "}\n",
    "\"\"\",\n",
    "    placeholder='Enter your SPARQL query here...',\n",
    "    description='SPARQL Query:',\n",
    "    layout=widgets.Layout(width='100%', height='150px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Buttons\n",
    "run_query_button = widgets.Button(description=\"Run Query\", button_style='primary')\n",
    "run_query_button.on_click(on_query_run)\n",
    "\n",
    "confirm_button = widgets.Button(description=\"Confirm selection\", button_style='success')\n",
    "confirm_button.on_click(on_confirm_selection)\n",
    "\n",
    "# Dropdown f√ºr Systeme\n",
    "system_selector = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description=\"Select system:\",\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Globales Ergebnis\n",
    "matched_systems = []\n",
    "\n",
    "\n",
    "# üß© Query-Komponenten anzeigen\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        query_input,\n",
    "        run_query_button,\n",
    "        query_interface_output,\n",
    "        widgets.HBox([system_selector, confirm_button]),\n",
    "        selection_output\n",
    "    ])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rdflib import Graph, Namespace, URIRef, RDF\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "json_output = widgets.Output()\n",
    "\n",
    "def extract_and_export_selected_system():\n",
    "    global selected_system_id, selected_system_uri, result_graph\n",
    "\n",
    "    SWEMLS = Namespace(\"https://w3id.org/semsys/ns/swemls#\")\n",
    "    OPMW = Namespace(\"http://www.opmw.org/ontology#\")\n",
    "    RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "\n",
    "    rdf_type = URIRef(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\")\n",
    "    ai_system_type = SWEMLS.System\n",
    "    documentation_type = SWEMLS.Documentation\n",
    "    paper_type = SWEMLS.Paper\n",
    "    ml_component_type = SWEMLS.MachineLearningComponent\n",
    "    kr_component_type = SWEMLS.KnowledgeRepresentationComponent\n",
    "    data_type = SWEMLS.Data\n",
    "    semantic_web_resource_type = SWEMLS.SemanticWebResource\n",
    "    has_documentation = SWEMLS.hasDocumentation\n",
    "    reports_on = SWEMLS.reports\n",
    "    rdfs_label = RDFS.label\n",
    "\n",
    "    g = result_graph\n",
    "\n",
    "    # 1. System und zugeh√∂rige Paper finden\n",
    "    ai_systems = [\n",
    "        system for system in g.subjects(RDF.type, ai_system_type)\n",
    "        if str(system).split(\"/\")[-1] == selected_system_id\n",
    "    ]\n",
    "\n",
    "    system_to_paper = {}\n",
    "    for paper in g.subjects(predicate=rdf_type, object=paper_type):\n",
    "        for reported_system in g.objects(subject=paper, predicate=reports_on):\n",
    "            system_id = str(reported_system).split(\"/\")[-1]\n",
    "            paper_id = str(paper).split(\"/\")[-1]\n",
    "            paper_metadata = {\"id\": paper_id, \"metadata\": {}}\n",
    "            for pred, obj in g.predicate_objects(subject=paper):\n",
    "                pred_name = pred.split(\"#\")[-1] if \"#\" in pred else pred.split(\"/\")[-1]\n",
    "                paper_metadata[\"metadata\"].setdefault(pred_name, []).append(str(obj))\n",
    "            system_to_paper.setdefault(system_id, []).append(paper_metadata)\n",
    "\n",
    "    # 2. System extrahieren\n",
    "    for system in ai_systems:\n",
    "        instance_data = {\n",
    "            \"id\": str(system).split(\"/\")[-1],\n",
    "            \"type\": \"System\",\n",
    "            \"metadata\": {},\n",
    "            \"relationships\": {},\n",
    "            \"documentation\": {},\n",
    "            \"papers\": [],\n",
    "            \"steps\": [],\n",
    "            \"variables\": {}\n",
    "        }\n",
    "\n",
    "        for pred, obj in g.predicate_objects(subject=system):\n",
    "            pred_name = pred.split(\"#\")[-1] if \"#\" in pred else pred.split(\"/\")[-1]\n",
    "            if isinstance(obj, URIRef):\n",
    "                instance_data[\"relationships\"].setdefault(pred_name, []).append(str(obj).split(\"/\")[-1])\n",
    "            else:\n",
    "                instance_data[\"metadata\"][pred_name] = str(obj)\n",
    "\n",
    "        # 3. Dokumentation extrahieren\n",
    "        if \"hasDocumentation\" in instance_data[\"relationships\"]:\n",
    "            for doc_id in instance_data[\"relationships\"][\"hasDocumentation\"]:\n",
    "                doc_uri = URIRef(f\"http://semantic-systems.net/swemls/{doc_id}\")\n",
    "                if (doc_uri, rdf_type, documentation_type) in g:\n",
    "                    instance_data[\"documentation\"][\"id\"] = doc_id\n",
    "                    for doc_pred, doc_obj in g.predicate_objects(subject=doc_uri):\n",
    "                        doc_pred_name = doc_pred.split(\"#\")[-1] if \"#\" in doc_pred else doc_pred.split(\"/\")[-1]\n",
    "                        instance_data[\"documentation\"][doc_pred_name] = str(doc_obj)\n",
    "\n",
    "        # 4. Paper hinzuf√ºgen\n",
    "        if selected_system_id in system_to_paper:\n",
    "            instance_data[\"papers\"] = system_to_paper[selected_system_id]\n",
    "\n",
    "        # 5. Schritte (ML/KR) extrahieren\n",
    "        for i in range(1, 99):\n",
    "            for step_relation in [f\"hasStepML{i}\", f\"hasStepKR{i}\"]:\n",
    "                if step_relation in instance_data[\"relationships\"]:\n",
    "                    for step_id in instance_data[\"relationships\"][step_relation]:\n",
    "                        step_uri = URIRef(f\"http://semantic-systems.net/swemls/{step_id}\")\n",
    "                        step_type = \"Unknown\"\n",
    "                        if (step_uri, rdf_type, ml_component_type) in g:\n",
    "                            step_type = \"Machine Learning\"\n",
    "                        elif (step_uri, rdf_type, kr_component_type) in g:\n",
    "                            step_type = \"Knowledge Representation\"\n",
    "                        step_data = {\"id\": step_id, \"type\": step_type, \"metadata\": {}}\n",
    "                        for step_pred, step_obj in g.predicate_objects(subject=step_uri):\n",
    "                            step_pred_name = step_pred.split(\"#\")[-1] if \"#\" in step_pred else step_pred.split(\"/\")[-1]\n",
    "                            step_data[\"metadata\"].setdefault(step_pred_name, []).append(str(step_obj))\n",
    "                        instance_data[\"steps\"].append(step_data)\n",
    "\n",
    "        # 6. Variablen extrahieren\n",
    "        for i in range(1, 99):\n",
    "            for var_relation in [f\"hasVariableData{i}\", f\"hasVariableSW{i}\"]:\n",
    "                if var_relation in instance_data[\"relationships\"]:\n",
    "                    for var_id in instance_data[\"relationships\"][var_relation]:\n",
    "                        var_uri = URIRef(f\"http://semantic-systems.net/swemls/{var_id}\")\n",
    "                        label = None\n",
    "                        for _, _, label_value in g.triples((var_uri, rdfs_label, None)):\n",
    "                            label = str(label_value)\n",
    "                            break\n",
    "                        instance_data[\"variables\"][var_relation] = {\"id\": var_id, \"label\": label}\n",
    "\n",
    "        # 7. Speichern\n",
    "        json_filename = f\"{selected_system_id}.json\"\n",
    "        with open(json_filename, \"w\") as f:\n",
    "            json.dump(instance_data, f, indent=4)\n",
    "\n",
    "        with json_output:\n",
    "            json_output.clear_output()\n",
    "            print(f\"‚úÖ JSON successfully exported as: {json_filename}\")\n",
    "            print(json.dumps(instance_data, indent=2))  # Ausgabe f√ºr √úberpr√ºfung\n",
    "\n",
    "# === Button zum Starten der Extraktion ===\n",
    "#extract_button = widgets.Button(description=\"Extract JSON\", button_style=\"success\")\n",
    "#extract_button.on_click(lambda b: extract_and_export_selected_system())\n",
    "\n",
    "#display(extract_button, json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "pattern_output = widgets.Output()\n",
    "\n",
    "# === Component Mapping f√ºr automatische Generierung oder Pattern-Upload ===\n",
    "import re\n",
    "import json\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "def run_component_mapping():\n",
    "    global components, json_to_pattern_map, edges\n",
    "\n",
    "    #display(\"üß© Starte Component Mapping ...\")\n",
    "\n",
    "    # Schritt 1: Mapping von JSON zu Pattern-K√ºrzeln (Verwendung der ID anstelle des Labels)\n",
    "    json_to_pattern_map = {}\n",
    "\n",
    "    for step in instance_data[\"steps\"]:\n",
    "        step_key = step[\"id\"].split(\".\")[-1]  # Extrahiere die ID des Schrittes\n",
    "        json_to_pattern_map[step_key] = step[\"id\"]  # Speichere die ID statt des Labels\n",
    "\n",
    "    for var_key, var_data in instance_data[\"variables\"].items():\n",
    "        json_to_pattern_map[var_key] = var_data[\"id\"]  # Verwende die ID der Variablen\n",
    "\n",
    "    for var_data in instance_data[\"variables\"].values():\n",
    "        key = var_data[\"id\"].split(\".\")[-1]\n",
    "        if key not in json_to_pattern_map:\n",
    "            json_to_pattern_map[key] = var_data[\"id\"]\n",
    "\n",
    "    # Schritt 2: Komponentenstruktur initialisieren\n",
    "    components = {\n",
    "        \"ml_component\": [],\n",
    "        \"kr_component\": [],\n",
    "        \"data\": [],\n",
    "        \"symbolic_data\": []\n",
    "    }\n",
    "\n",
    "    for step in instance_data[\"steps\"]:\n",
    "        step_type_list = step.get(\"metadata\", {}).get(\"type\", [])\n",
    "        step_type = step_type_list[-1] if step_type_list else \"\"\n",
    "        if \"MachineLearningComponent\" in step_type and step[\"id\"] not in components[\"ml_component\"]:\n",
    "            components[\"ml_component\"].append(step[\"id\"])\n",
    "        elif \"KnowledgeRepresentationComponent\" in step_type and step[\"id\"] not in components[\"kr_component\"]:\n",
    "            components[\"kr_component\"].append(step[\"id\"])\n",
    "\n",
    "    for var_relation, var_data in instance_data[\"variables\"].items():\n",
    "        if \"SW\" in var_relation:\n",
    "            components[\"symbolic_data\"].append(var_data[\"id\"])\n",
    "        else:\n",
    "            components[\"data\"].append(var_data[\"id\"])\n",
    "\n",
    "    # Schritt 3: Fehlende Variablen im Pattern erg√§nzen\n",
    "    for key in set(json_to_pattern_map.keys()) - set(pattern_structure[\"variables\"].keys()):\n",
    "        pattern_structure[\"variables\"][key] = {\"generated_by\": []}\n",
    "\n",
    "    # Schritt 4: Letzter Check auf fehlende Nodes\n",
    "    missing_nodes = set(json_to_pattern_map.keys()) - set(pattern_structure[\"variables\"].keys()) - set(pattern_structure[\"steps\"].keys())\n",
    "    if missing_nodes:\n",
    "        display(f\" ‚ö†Ô∏è WARNING: Nodes still missing after fix: {missing_nodes}\")\n",
    "\n",
    "    # === Edge Mapping ===\n",
    "    #display(\"\\nüîó Starte Edge Mapping...\")\n",
    "\n",
    "    edges = []  # Initialize edges list\n",
    "\n",
    "    # Dynamische Zuordnung spezieller Variablen\n",
    "    special_mappings = {f\"Data{i}\": f\"hasVariableData{i}\" for i in range(1, 99)}\n",
    "    special_mappings.update({f\"SW{i}\": f\"hasVariableSW{i}\" for i in range(1, 99)})\n",
    "\n",
    "    # Fehlende Pattern-Variablen erg√§nzen\n",
    "    for var_name in pattern_structure[\"variables\"]:\n",
    "        if var_name not in json_to_pattern_map:\n",
    "            found_match = False\n",
    "\n",
    "            if var_name in special_mappings:\n",
    "                mapped_var = instance_data[\"variables\"].get(special_mappings[var_name], {}).get(\"id\")\n",
    "                if mapped_var:\n",
    "                    json_to_pattern_map[var_name] = mapped_var\n",
    "                    found_match = True\n",
    "\n",
    "            if not found_match:\n",
    "                for var_key, var_data in instance_data[\"variables\"].items():\n",
    "                    if var_name.lower() in var_key.lower():\n",
    "                        json_to_pattern_map[var_name] = var_data[\"id\"]\n",
    "                        found_match = True\n",
    "                        break\n",
    "\n",
    "            if not found_match:\n",
    "                for rel_key, rel_values in instance_data[\"relationships\"].items():\n",
    "                    if isinstance(rel_values, list):\n",
    "                        for value in rel_values:\n",
    "                            if var_name.lower() == value.split(\".\")[-1].lower():\n",
    "                                json_to_pattern_map[var_name] = value\n",
    "                                found_match = True\n",
    "                                break\n",
    "                    if found_match:\n",
    "                        break\n",
    "\n",
    "            if not found_match:\n",
    "                json_to_pattern_map[var_name] = var_name\n",
    "\n",
    "    # Kanten generieren (Hier wird sichergestellt, dass alle echten IDs verwendet werden)\n",
    "    for step, relations in pattern_structure[\"steps\"].items():\n",
    "        step_real_id = json_to_pattern_map.get(step, step)\n",
    "\n",
    "        # \"used\"-Kanten erstellen\n",
    "        for used in relations.get(\"uses\", []):\n",
    "            used_real_id = json_to_pattern_map.get(used, used)\n",
    "            if used_real_id and step_real_id:\n",
    "                edges.append((used_real_id, step_real_id))\n",
    "\n",
    "        # \"output\"-Kanten erstellen\n",
    "        for output in relations.get(\"outputs\", []):\n",
    "            output_real_id = json_to_pattern_map.get(output, output)\n",
    "            if output_real_id and step_real_id:\n",
    "                edges.append((step_real_id, output_real_id))\n",
    "\n",
    "    #display(f\"\\n‚úÖ Final Edge List: {edges}\")\n",
    "\n",
    "    # Pr√ºfen auf unverbundene Knoten\n",
    "    all_edge_nodes = {node for edge in edges for node in edge}\n",
    "    unlinked_nodes = set(json_to_pattern_map.values()) - all_edge_nodes\n",
    "    if unlinked_nodes:\n",
    "        display(f\"‚ö†Ô∏è WARNING: Nodes in mapping but not used in edges: {unlinked_nodes}\")\n",
    "\n",
    "    # Ausgabe der components-Struktur\n",
    "    #display(\"\\nüß© Components Struktur:\")\n",
    "    #display(components)\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "def assign_ranks_with_limited_correction(components, edges):\n",
    "    #print(\"üîß Starte Topological Rank Assignment mit begrenzter Korrektur (nur bei Knoten ohne Vorg√§nger)...\\n\")\n",
    "\n",
    "    # Schritt 1: Graph konstruieren\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    if not nx.is_directed_acyclic_graph(G):\n",
    "        print(\"‚ùå Error: Graph contains cycle.\")\n",
    "        return None\n",
    "\n",
    "    topo_order = list(nx.topological_sort(G))\n",
    "\n",
    "    # Schritt 2: Initiale Rangzuweisung\n",
    "    rank_map = {}\n",
    "    for node in topo_order:\n",
    "        preds = list(G.predecessors(node))\n",
    "\n",
    "        if not preds:\n",
    "            rank_map[node] = 0\n",
    "        else:\n",
    "            rank_map[node] = max(rank_map[p] + 1 for p in preds)\n",
    "\n",
    "    #print(\"üéØ Vorl√§ufige R√§nge:\")\n",
    "    #for node, rank in rank_map.items():\n",
    "        #print(f\"  {node}: Rang {rank}\")\n",
    "\n",
    "    # Schritt 3: Korrektur nur f√ºr Knoten ohne Vorg√§nger\n",
    "    #print(\"\\nüîÅ Starte begrenzte Shared-Input-Korrektur f√ºr Knoten ohne Vorg√§nger...\")\n",
    "    \n",
    "    all_targets = defaultdict(list)\n",
    "    for source, target in edges:\n",
    "        all_targets[source].append(target)\n",
    "\n",
    "    for node, targets in all_targets.items():\n",
    "        if list(G.predecessors(node)):  # Skip nodes with predecessors\n",
    "            continue\n",
    "\n",
    "        candidate_ranks = []\n",
    "        for target in targets:\n",
    "            target_rank = rank_map.get(target, 1)\n",
    "            candidate_ranks.append(target_rank - 1)\n",
    "\n",
    "        if candidate_ranks:\n",
    "            new_rank = min(candidate_ranks)\n",
    "            if new_rank >= 0 and new_rank != rank_map[node]:\n",
    "                #print(f\"  üîÅ Korrektur: {node} von Rang {rank_map[node]} ‚Üí {new_rank} (basierend auf {len(candidate_ranks)} Targets)\")\n",
    "                rank_map[node] = new_rank\n",
    "\n",
    "    # Schritt 4: Gruppierung nach R√§ngen\n",
    "    grouped = defaultdict(list)\n",
    "    for node, rank in rank_map.items():\n",
    "        grouped[rank].append(node)\n",
    "\n",
    "    #print(\"\\nüìä Final zugewiesene R√§nge nach Korrektur:\")\n",
    "    #for rank in sorted(grouped):\n",
    "        #print(f\"  Rang {rank}: {grouped[rank]}\")\n",
    "\n",
    "    return rank_map\n",
    "\n",
    "\n",
    "def apply_positioning_algorithm():\n",
    "    global positions  # wichtig, damit du sp√§ter darauf zugreifen kannst\n",
    "\n",
    "    if components and edges:\n",
    "        #print(\"üöÄ Starte Positionierungsalgorithmus...\")\n",
    "\n",
    "        rank_map = assign_ranks_with_limited_correction(components, edges)\n",
    "\n",
    "        if rank_map:\n",
    "            #print(\"\\nüìê Rangzuweisung erfolgreich.\")\n",
    "\n",
    "            if pattern_choice in [\"generate\", \"upload\"]:\n",
    "                #print(\"üîÅ Wende Median-Heuristik an...\")\n",
    "                ordered_layers = apply_median_heuristic(rank_map, edges, sort_layer_0=True)\n",
    "\n",
    "                #print(\"üß© Starte Greedy-Swap...\")\n",
    "                optimized_layers = greedy_swap_optimization(ordered_layers, edges)\n",
    "\n",
    "                #print(\"üìç Koordinaten zuweisen...\")\n",
    "                positions = assign_coordinates(optimized_layers)\n",
    "\n",
    "                #print(\"\\nüìç Initiale Koordinaten:\")\n",
    "                #for node, pos in positions.items():\n",
    "                    #print(f\"  {node}: {pos}\")\n",
    "\n",
    "                #print(\"üß† Letzter √úberlappungs-Fix...\")\n",
    "                positions = simple_overlap_correction(positions, edges)\n",
    "\n",
    "                #print(\"\\nüìç Final zugewiesene Koordinaten:\")\n",
    "                #for node, pos in positions.items():\n",
    "                    #print(f\"  {node}: {pos}\")\n",
    "                finalize_layout_parameters()  # ‚¨ÖÔ∏è hier den finalen Check aufrufen\n",
    "\n",
    "\n",
    "                return rank_map\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Rank assigment unsuccessful.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Component or Edge missing\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Funktion zur ID-Deduplizierung ===\n",
    "def deduplicate_ids(variables):\n",
    "    seen_ids = {}\n",
    "    updated_variables = {}\n",
    "\n",
    "    for key, var in variables.items():\n",
    "        original_id = var[\"id\"]\n",
    "        label = var.get(\"label\", \"\")\n",
    "\n",
    "        if original_id in seen_ids:\n",
    "            seen_ids[original_id] += 1\n",
    "            new_id = f\"{original_id}_{seen_ids[original_id]}\"\n",
    "        else:\n",
    "            seen_ids[original_id] = 1\n",
    "            new_id = original_id\n",
    "\n",
    "        updated_variables[key] = {\n",
    "            \"id\": new_id,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "    return updated_variables\n",
    "\n",
    "# === JSON-Datei einlesen und Variablen setzen ===\n",
    "def load_instance_json(json_filename):\n",
    "    global instance_data, pattern, pattern_structure  # üÜï pattern_structure erg√§nzt\n",
    "\n",
    "    try:\n",
    "        with open(json_filename, \"r\") as json_file:\n",
    "            extracted_data = json.load(json_file)\n",
    "            instance_data = extracted_data\n",
    "\n",
    "        # üßπ Wichtig: Alte pattern_structure zur√ºcksetzen\n",
    "        pattern_structure = None\n",
    "\n",
    "        # System-Label und Pattern-URI extrahieren\n",
    "        system_label = instance_data.get(\"metadata\", {}).get(\"label\", \"no label\")\n",
    "        raw_pattern_uri = instance_data.get(\"relationships\", {}).get(\"hasCorrespondingPattern\", [None])[0]\n",
    "\n",
    "        # Sicher extrahieren ‚Äì falls kein Pattern vorhanden, als \"no pattern\" setzen\n",
    "        if raw_pattern_uri:\n",
    "            pattern = raw_pattern_uri.split(\".\")[-1] if \"Pattern.\" in raw_pattern_uri else raw_pattern_uri.split(\"/\")[-1]\n",
    "        else:\n",
    "            pattern = \"no pattern\"\n",
    "\n",
    "        # IDs deduplizieren\n",
    "        instance_data[\"variables\"] = deduplicate_ids(instance_data.get(\"variables\", {}))\n",
    "\n",
    "        # Danach weitere Pattern-Entscheidung anzeigen\n",
    "        handle_pattern_selection()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        with pattern_output:\n",
    "            pattern_output.clear_output()\n",
    "            print(\"‚ùå JSON file not found! Please extract an instance first.\")\n",
    "    except Exception as e:\n",
    "        with pattern_output:\n",
    "            pattern_output.clear_output()\n",
    "            print(f\"‚ùå Error loading JSON:\\n‚Üí {e}\")\n",
    "\n",
    "\n",
    "# === Button zur Ausf√ºhrung ===\n",
    "#load_button = widgets.Button(description=\"Load Extracted JSON\", button_style=\"primary\")\n",
    "\n",
    "#def on_load_click(b):\n",
    " #   try:\n",
    "  #      json_file = f\"{selected_system_id}.json\"\n",
    "   #     load_instance_json(json_file)\n",
    "    #except NameError:\n",
    "      #  with pattern_output:\n",
    "     #       pattern_output.clear_output()\n",
    "       #     print(\"‚ùå No system selected. Please extract a system first.\")\n",
    "\n",
    "#load_button.on_click(on_load_click)\n",
    "\n",
    "\n",
    "#display(load_button, pattern_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from rdflib import Graph, Namespace, RDF\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# === Globals for reuse ===\n",
    "pattern_structure = None\n",
    "pattern_choice = None\n",
    "pattern_graph = None\n",
    "pattern = None\n",
    "\n",
    "# === Variables from previous steps assumed ===\n",
    "# instance_data (dict), pattern (str or \"no pattern\")\n",
    "\n",
    "# === Output containers ===\n",
    "pattern_output = widgets.Output()\n",
    "pattern_upload_output = widgets.Output()\n",
    "\n",
    "download_button = widgets.Button(\n",
    "    description=\"‚¨áÔ∏è Download Template XML\",\n",
    "    button_style=\"info\",\n",
    "    icon=\"download\"\n",
    ")\n",
    "download_output = widgets.Output()\n",
    "\n",
    "\n",
    "pattern_continue_button = widgets.Button(\n",
    "    description=\"Continue with uploaded pattern\",\n",
    "    button_style=\"success\"\n",
    ")\n",
    "\n",
    "# === Upload widget (only shown when needed) ===\n",
    "pattern_upload_widget = widgets.FileUpload(\n",
    "    accept='.ttl',\n",
    "    multiple=False,\n",
    "    description='Upload Pattern File'\n",
    ")\n",
    "\n",
    "# === Pattern decision: Template or Generate ===\n",
    "pattern_choice_selector = widgets.ToggleButtons(\n",
    "    options=[\n",
    "        (\"Use template\", \"template\"),\n",
    "        (\"Generate automatically\", \"generate\")\n",
    "    ],\n",
    "    description=\"Select Pattern Option:\",\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "\n",
    "def generate_template_from_instance():\n",
    "    global pattern, instance_data, pattern_structure\n",
    "\n",
    "    with download_output:\n",
    "        download_output.clear_output()\n",
    "        try:\n",
    "            #print(\"üõ† generate_template_from_instance() aufgerufen\")\n",
    "            #print(f\"üîç Aktuelles Pattern: {pattern}\")\n",
    "            #print(f\"üì¶ instance_data vorhanden: {'instance_data' in globals()}\")\n",
    "            #print(f\"üì¶ pattern_structure vorhanden: {pattern_structure is not None}\")\n",
    "\n",
    "            if not pattern or pattern in (\"no pattern\", \"None\", None):\n",
    "                print(\"‚ö†Ô∏è No Pattern available!\")\n",
    "                return\n",
    "\n",
    "            # Falls pattern_structure fehlt: nachladen\n",
    "            if not pattern_structure:\n",
    "                pattern_file_path = f\"Patterns/{pattern}-pattern.ttl\"\n",
    "                if os.path.exists(pattern_file_path):\n",
    "                    #print(f\"üìÑ Lade Pattern-Datei: {pattern_file_path}\")\n",
    "                    pattern_structure = extract_pattern_structure_from_file(pattern_file_path)\n",
    "                else:\n",
    "                    print(f\"‚ùå Pattern-file missing: {pattern_file_path}\")\n",
    "                    return\n",
    "\n",
    "            if not pattern_structure or not isinstance(pattern_structure, dict):\n",
    "                print(\"‚ùå Pattern file missing or empty pattern_structure\")\n",
    "                return\n",
    "\n",
    "            #print(f\"üìä pattern_structure preview:\")\n",
    "            #print(json.dumps(pattern_structure, indent=2))\n",
    "\n",
    "            # Template laden\n",
    "            template_path = f\"Templates/{pattern}.xml\"\n",
    "            if not os.path.exists(template_path):\n",
    "                print(f\"‚ùå Template-file missing: {template_path}\")\n",
    "                return\n",
    "\n",
    "            with open(template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                xml_template = f.read()\n",
    "                #print(f\"‚úÖ Template geladen ({len(xml_template)} Zeichen)\")\n",
    "\n",
    "            updated_xml = replace_pattern_placeholders(xml_template, instance_data, pattern_structure)\n",
    "\n",
    "            if not updated_xml.strip():\n",
    "                print(\"‚ùå Generated XML ist empty\")\n",
    "                return\n",
    "\n",
    "            instance_id = instance_data.get(\"id\", \"unknown\")\n",
    "            filename = f\"Updated_{instance_id}_Workflow.xml\"\n",
    "\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(updated_xml)\n",
    "                print(f\"‚úÖ XML saved: {filename}\")\n",
    "\n",
    "            b64 = base64.b64encode(updated_xml.encode()).decode()\n",
    "            href = f'data:application/xml;base64,{b64}'\n",
    "            display(HTML(f'<a download=\"{filename}\" href=\"{href}\" target=\"_blank\">‚¨áÔ∏è Click here to download the XML file</a>'))\n",
    "\n",
    "            # Starte Merge\n",
    "            finalize_and_merge_xml(filename)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error while generating template\")\n",
    "            traceback.print_exception(type(e), e, e.__traceback__)\n",
    "\n",
    "\n",
    "def handle_pattern_selection():\n",
    "    global pattern, pattern_choice, pattern_structure\n",
    "\n",
    "    pattern_output.clear_output()\n",
    "    pattern_upload_output.clear_output()\n",
    "\n",
    "    try:\n",
    "        with pattern_output:\n",
    "            print(f\"üîç pattern: {pattern}\")\n",
    "\n",
    "            if pattern is None or pattern == \"no pattern\":\n",
    "                print(\"üìÇ No pattern linked to system. Please upload a pattern TTL file.\")\n",
    "                display(pattern_upload_widget)\n",
    "                display(pattern_continue_button)\n",
    "                display(pattern_upload_output)\n",
    "            else:\n",
    "                pattern_file_path = f\"Patterns/{pattern}-pattern.ttl\"\n",
    "                if not os.path.exists(pattern_file_path):\n",
    "                    print(f\"‚ö†Ô∏è Pattern-file for {pattern} not found.\")\n",
    "                    print(\"üì§ Please upload valid pattern file.\")\n",
    "                    display(pattern_upload_widget)\n",
    "                    display(pattern_continue_button)\n",
    "                    display(pattern_upload_output)\n",
    "                else:\n",
    "                    print(f\"üß© Pattern detected: {pattern}\")\n",
    "                    print(\"How would you like to proceed?\")\n",
    "                    display(pattern_choice_selector)\n",
    "\n",
    "                    # Nur anzeigen, wenn nicht bereits sichtbar\n",
    "                    if pattern_decision_continue_button.layout.display != \"inline-block\":\n",
    "                        pattern_decision_continue_button.layout.display = \"inline-block\"\n",
    "\n",
    "    except Exception as e:\n",
    "        show_exception(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def on_pattern_continue(b):\n",
    "    global pattern_structure, pattern_choice, pattern\n",
    "    pattern_structure = None\n",
    "\n",
    "    with pattern_output:\n",
    "        pattern_output.clear_output()\n",
    "        #print(\"üü¢ [DEBUG] Button clicked!\")\n",
    "\n",
    "        if not pattern_upload_widget.value:\n",
    "            print(\"‚ö†Ô∏è No file uploaded.\")\n",
    "            return\n",
    "\n",
    "        upload_info = pattern_upload_widget.value[0]\n",
    "        file_name = upload_info['name']\n",
    "        file_content = upload_info['content']\n",
    "\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(file_content)\n",
    "\n",
    "        #print(f\"üü¢ [DEBUG] File '{file_name}' written to disk.\")\n",
    "\n",
    "        try:\n",
    "            pattern_structure = extract_pattern_structure_from_file(file_name)\n",
    "            #print(\"‚úÖ Pattern structure parsed.\")\n",
    "\n",
    "            pattern_choice = \"upload\"\n",
    "            pattern = \"custom\"\n",
    "\n",
    "            # Sofort generieren und mergen!\n",
    "            run_component_mapping()\n",
    "            apply_positioning_algorithm()\n",
    "\n",
    "            instance_id = instance_data.get(\"id\", \"unknown\")\n",
    "            workflow_path = f\"Generated_{instance_id}_Workflow.drawio\"\n",
    "\n",
    "            xml_output = generate_drawio_xml(positions, components, edges, label_map)\n",
    "\n",
    "            with open(workflow_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(xml_output)\n",
    "\n",
    "            # Download-Link erzeugen\n",
    "            b64 = base64.b64encode(xml_output.encode()).decode()\n",
    "            href = f'data:application/xml;base64,{b64}'\n",
    "            #display(HTML(f'<a download=\"{workflow_path}\" href=\"{href}\" target=\"_blank\">‚¨áÔ∏è Click here to download the XML file</a>'))\n",
    "\n",
    "            #print(f\"üì• File ready: {workflow_path}\")\n",
    "\n",
    "            finalize_and_merge_xml(workflow_path)\n",
    "\n",
    "            # üîÅ Kein weiterer Button n√∂tig!\n",
    "            pattern_decision_continue_button.layout.display = \"none\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing pattern file or generating XML: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Am Ende registrieren:\n",
    "pattern_continue_button.on_click(on_pattern_continue)\n",
    "\n",
    "\n",
    "# === Extract logic from TTL file ===\n",
    "def extract_pattern_structure_from_file(file_path):\n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=\"turtle\")\n",
    "\n",
    "    SWEMLS = Namespace(\"https://w3id.org/semsys/ns/swemls#\")\n",
    "    OPMW = Namespace(\"http://www.opmw.org/ontology/\")\n",
    "    RES = Namespace(\"http://semantic-systems.net/swemls/\")\n",
    "\n",
    "    def clean_uri(uri):\n",
    "        label = uri.split(\"/\")[-1].split(\"#\")[-1]\n",
    "        label = re.sub(r'Pattern\\.[A-Za-z0-9]+\\.', '', label)\n",
    "        label = re.sub(r'^[A-Za-z0-9]+\\.', '', label)\n",
    "        return label\n",
    "\n",
    "    structure = {\"steps\": {}, \"variables\": {}}\n",
    "\n",
    "    for step in g.subjects(RDF.type, SWEMLS.WorkflowTemplateProcessML):\n",
    "        step_label = clean_uri(str(step))\n",
    "        inputs = [clean_uri(str(var)) for var in g.objects(step, OPMW[\"uses\"])]\n",
    "        structure[\"steps\"][step_label] = {\"type\": \"ML\", \"uses\": inputs, \"outputs\": []}\n",
    "\n",
    "    for step in g.subjects(RDF.type, SWEMLS.WorkflowTemplateProcessKR):\n",
    "        step_label = clean_uri(str(step))\n",
    "        inputs = [clean_uri(str(var)) for var in g.objects(step, OPMW[\"uses\"])]\n",
    "        structure[\"steps\"][step_label] = {\"type\": \"KR\", \"uses\": inputs, \"outputs\": []}\n",
    "\n",
    "    for step in g.subjects(RDF.type, RES.WorkflowTemplateProcessML):\n",
    "        step_label = clean_uri(str(step))\n",
    "        inputs = [clean_uri(str(var)) for var in g.objects(step, OPMW[\"uses\"])]\n",
    "        structure[\"steps\"][step_label] = {\"type\": \"ML\", \"uses\": inputs, \"outputs\": []}\n",
    "\n",
    "    for step in g.subjects(RDF.type, RES.WorkflowTemplateProcessKR):\n",
    "        step_label = clean_uri(str(step))\n",
    "        inputs = [clean_uri(str(var)) for var in g.objects(step, OPMW[\"uses\"])]\n",
    "        structure[\"steps\"][step_label] = {\"type\": \"KR\", \"uses\": inputs, \"outputs\": []}\n",
    "\n",
    "    for var in g.subjects(RDF.type, SWEMLS.TemplateArtifactData):\n",
    "        var_label = clean_uri(str(var))\n",
    "        generated_by = [clean_uri(str(gen)) for gen in g.objects(var, OPMW[\"isGeneratedBy\"])]\n",
    "        for gen in generated_by:\n",
    "            if gen in structure[\"steps\"]:\n",
    "                structure[\"steps\"][gen][\"outputs\"].append(var_label)\n",
    "        structure[\"variables\"][var_label] = {\"generated_by\": generated_by} if generated_by else {}\n",
    "\n",
    "    for var in g.subjects(RDF.type, RES.TemplateArtifactData):\n",
    "        var_label = clean_uri(str(var))\n",
    "        generated_by = [clean_uri(str(gen)) for gen in g.objects(var, OPMW[\"isGeneratedBy\"])]\n",
    "        for gen in generated_by:\n",
    "            if gen in structure[\"steps\"]:\n",
    "                structure[\"steps\"][gen][\"outputs\"].append(var_label)\n",
    "        structure[\"variables\"][var_label] = {\"generated_by\": generated_by} if generated_by else {}\n",
    "\n",
    "    for var in g.subjects(RDF.type, SWEMLS.TemplateArtifactSW):\n",
    "        var_label = clean_uri(str(var))\n",
    "        generated_by = [clean_uri(str(gen)) for gen in g.objects(var, OPMW[\"isGeneratedBy\"])]\n",
    "        for gen in generated_by:\n",
    "            if gen in structure[\"steps\"]:\n",
    "                structure[\"steps\"][gen][\"outputs\"].append(var_label)\n",
    "        structure[\"variables\"][var_label] = {\"generated_by\": generated_by} if generated_by else {}\n",
    "\n",
    "    for var in g.subjects(RDF.type, RES.TemplateArtifactSW):\n",
    "        var_label = clean_uri(str(var))\n",
    "        generated_by = [clean_uri(str(gen)) for gen in g.objects(var, OPMW[\"isGeneratedBy\"])]\n",
    "        for gen in generated_by:\n",
    "            if gen in structure[\"steps\"]:\n",
    "                structure[\"steps\"][gen][\"outputs\"].append(var_label)\n",
    "        structure[\"variables\"][var_label] = {\"generated_by\": generated_by} if generated_by else {}\n",
    "\n",
    "    return structure\n",
    "\n",
    "# === Trigger on pattern upload ===\n",
    "def on_pattern_upload(change):\n",
    "    global pattern_structure\n",
    "\n",
    "    if not pattern_upload_widget.value:\n",
    "        return\n",
    "\n",
    "    uploaded = next(iter(pattern_upload_widget.value.items()))  # (filename, fileinfo)\n",
    "    file_name = uploaded[0]\n",
    "    file_content = uploaded[1]['content']\n",
    "\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "\n",
    "    with pattern_output:\n",
    "        clear_output()\n",
    "        print(f\"‚úÖ Pattern file uploaded: {file_name}\")\n",
    "\n",
    "    try:\n",
    "        pattern_structure = extract_pattern_structure_from_file(file_name)\n",
    "        with pattern_output:\n",
    "            print(\"‚úÖ Extracted pattern structure:\")\n",
    "            print(json.dumps(pattern_structure, indent=4))\n",
    "    except Exception as e:\n",
    "        with pattern_output:\n",
    "            print(f\"‚ùå Error parsing pattern file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pattern_upload_widget.observe(on_pattern_upload, names='value')\n",
    "\n",
    "\n",
    "\n",
    "def on_pattern_choice(change):\n",
    "    global pattern_choice\n",
    "    pattern_choice = change['new']\n",
    "    # Kein sofortiger Code-Start!\n",
    "\n",
    "\n",
    "pattern_choice_selector.observe(on_pattern_choice, names='value')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Danach Anzeige und Auswahl starten\n",
    "main_pattern_box = widgets.VBox([\n",
    "    pattern_output,\n",
    "    pattern_upload_output,\n",
    "    download_output  # üÜï Zeigt den Download-Link nach Template-Erstellung\n",
    "])\n",
    "\n",
    "\n",
    "# Button-Handler aktivieren ‚¨áÔ∏è\n",
    "pattern_continue_button.on_click(on_pattern_continue)\n",
    "\n",
    "# Danach Anzeige und Auswahl starten\n",
    "handle_pattern_selection()\n",
    "display(main_pattern_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "import os\n",
    "import base64\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# === Globale Ausgabe-Widgets ===\n",
    "download_output = widgets.Output()\n",
    "\n",
    "# === Funktion zur Fehlerausgabe ===\n",
    "def show_exception(e):\n",
    "    print(\"‚ùå Exception caught:\")\n",
    "    traceback.print_exception(type(e), e, e.__traceback__, file=sys.stdout)\n",
    "\n",
    "# === Funktion zum Extrahieren des Labels aus einem Schritt ===\n",
    "def extract_label_short(step):\n",
    "    labels = step.get(\"metadata\", {}).get(\"label\", [])\n",
    "    if labels:\n",
    "        match = re.search(r\"\\((.*?)\\)\", labels[0])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return step[\"id\"].split(\".\")[-1]\n",
    "\n",
    "# === Funktion zum Ersetzen der Platzhalter im XML ===\n",
    "def replace_pattern_placeholders(xml_string, instance_data, pattern_structure):\n",
    "    #with download_output:\n",
    "     #   print(\"\\U0001f527 replace_pattern_placeholders() aufgerufen\")\n",
    "      #  print(f\"\\U0001f527 XML L√§nge: {len(xml_string)}\")\n",
    "       # print(f\"\\U0001f527 Instance data keys: {list(instance_data.keys())}\")\n",
    "        #print(f\"\\U0001f527 Pattern structure keys: {list(pattern_structure.keys())}\")\n",
    "\n",
    "    tree = ET.ElementTree(ET.fromstring(xml_string))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    replacements = {}\n",
    "\n",
    "    #with download_output:\n",
    "        #print(\"\\U0001f501 Schritte werden ersetzt...\")\n",
    "    for step in instance_data.get(\"steps\", []):\n",
    "        step_key = step[\"id\"].split(\".\")[-1]\n",
    "        step_label = extract_label_short(step)\n",
    "        replacements[step_key] = step_label\n",
    "\n",
    "    #with download_output:\n",
    "        #print(\"\\U0001f501 Variablen werden ersetzt...\")\n",
    "    for pattern_var in pattern_structure.get(\"variables\", {}):\n",
    "        for var_key, var_data in instance_data.get(\"variables\", {}).items():\n",
    "            if pattern_var.lower() in var_key.lower():\n",
    "                inst_id = var_data[\"id\"].replace(\"Resource.\", \"\").replace(\"Custom.\", \"\")\n",
    "                replacements[pattern_var] = inst_id\n",
    "                break\n",
    "\n",
    "    #with download_output:\n",
    "        #print(\"\\n\\U0001f9e9 Mapping for replacements:\")\n",
    "        #for k, v in replacements.items():\n",
    "           # print(f\"  {k} ‚Üí {v}\")\n",
    "\n",
    "    for element in root.iter(\"mxCell\"):\n",
    "        if 'value' in element.attrib:\n",
    "            value = element.attrib['value']\n",
    "            for placeholder, real_value in replacements.items():\n",
    "                if placeholder in value:\n",
    "                    replacement = real_value.replace(\"_\", \" \") if real_value.strip() else \"Missing\"\n",
    "                    style = element.attrib.get(\"style\", \"\")\n",
    "                    length = len(replacement)\n",
    "                    if length > 24:\n",
    "                        style += \";fontSize=5\"\n",
    "                    elif length > 20:\n",
    "                        style += \";fontSize=7\"\n",
    "                    elif length > 17:\n",
    "                        style += \";fontSize=9\"\n",
    "                    element.attrib['style'] = style\n",
    "                    element.attrib['value'] = value.replace(placeholder, replacement)\n",
    "\n",
    "    return ET.tostring(root, encoding='utf-8', method='xml').decode()\n",
    "\n",
    "# === Template generieren und Base64-Download-Link anzeigen ===\n",
    "def generate_template_from_instance():\n",
    "    global pattern, instance_data, pattern_structure\n",
    "\n",
    "    with download_output:\n",
    "        download_output.clear_output()\n",
    "        try:\n",
    "            #print(\"üõ† generate_template_from_instance() aufgerufen\")\n",
    "            #print(f\"üîç Aktuelles Pattern: {pattern}\")\n",
    "            #print(f\"üì¶ instance_data vorhanden: {'instance_data' in globals()}\")\n",
    "            #print(f\"üì¶ pattern_structure vorhanden: {pattern_structure is not None}\")\n",
    "\n",
    "            if not pattern or pattern in (\"no pattern\", \"None\", None):\n",
    "                print(\"‚ö†Ô∏è No valid pattern\")\n",
    "                return\n",
    "\n",
    "            # Falls pattern_structure fehlt: nachladen\n",
    "            if not pattern_structure:\n",
    "                pattern_file_path = f\"Patterns/{pattern}-pattern.ttl\"\n",
    "                if os.path.exists(pattern_file_path):\n",
    "                    #print(f\"üìÑ Lade Pattern-Datei: {pattern_file_path}\")\n",
    "                    pattern_structure = extract_pattern_structure_from_file(pattern_file_path)\n",
    "                else:\n",
    "                    print(f\"‚ùå Pattern-file missing: {pattern_file_path}\")\n",
    "                    return\n",
    "\n",
    "            if not pattern_structure or not isinstance(pattern_structure, dict):\n",
    "                print(\"‚ùå Invalid or empty pattern_structure\")\n",
    "                return\n",
    "\n",
    "            #print(f\"üìä pattern_structure preview:\")\n",
    "            #print(json.dumps(pattern_structure, indent=2))\n",
    "\n",
    "            # Template laden\n",
    "            template_path = f\"Templates/{pattern}.xml\"\n",
    "            if not os.path.exists(template_path):\n",
    "                print(f\"‚ùå Template file not found: {template_path}\")\n",
    "                return\n",
    "\n",
    "            with open(template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                xml_template = f.read()\n",
    "                #print(f\"‚úÖ Template-Datei geladen mit L√§nge {len(xml_template)}\")\n",
    "\n",
    "            updated_xml = replace_pattern_placeholders(xml_template, instance_data, pattern_structure)\n",
    "\n",
    "            if not updated_xml.strip():\n",
    "                print(\"‚ùå Generated XML is empty\")\n",
    "                return\n",
    "\n",
    "            instance_id = instance_data.get(\"id\", \"unknown\")\n",
    "            custom_filename = f\"Updated_{instance_id}_Workflow.xml\"\n",
    "\n",
    "            # 1. Speichern f√ºr den Download\n",
    "            with open(custom_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(updated_xml)\n",
    "                #print(f\"‚úÖ XML gespeichert unter: {custom_filename}\")\n",
    "\n",
    "            # 2. Speichern unter fixed Name f√ºr den Merge\n",
    "            with open(\"Generated_Workflow.drawio\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(updated_xml)\n",
    "                #print(\"‚úÖ XML auch unter Generated_Workflow.drawio gespeichert (f√ºr Merge)\")\n",
    "\n",
    "            # Download-Link erzeugen\n",
    "            b64 = base64.b64encode(updated_xml.encode()).decode()\n",
    "            href = f'data:application/xml;base64,{b64}'\n",
    "            display(HTML(f'<a download=\"{custom_filename}\" href=\"{href}\" target=\"_blank\">‚¨áÔ∏è Click here to download the XML file</a>'))\n",
    "\n",
    "            print(f\"üì• File ready: {custom_filename}\")\n",
    "\n",
    "            # üß© Merge starten mit fixer Datei\n",
    "            finalize_and_merge_xml(\"Generated_Workflow.drawio\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error while generating template\")\n",
    "            traceback.print_exception(type(e), e, e.__traceback__)\n",
    "\n",
    "# üÜï Entscheidung durchf√ºhren Button\n",
    "pattern_decision_continue_button = widgets.Button(\n",
    "    description=\"Continue with selected option\",\n",
    "    button_style=\"primary\",\n",
    "    icon=\"check\"\n",
    ")\n",
    "\n",
    "\n",
    "def on_pattern_decision_continue(b):\n",
    "    global pattern_choice, pattern_structure, pattern\n",
    "\n",
    "    # Ersten Wert √ºbernehmen\n",
    "    pattern_choice = pattern_choice_selector.value\n",
    "\n",
    "    # ‚õëÔ∏è Fallback: Wenn Upload aktiv war, √ºberschreibe Auswahl\n",
    "    if pattern_upload_widget.value and pattern == \"custom\":\n",
    "        pattern_choice = \"upload\"\n",
    "\n",
    "    with download_output:\n",
    "        download_output.clear_output()\n",
    "        print(\"‚û°Ô∏è Continue decision clicked\")\n",
    "        print(f\"üß≠ Selected option (final): {pattern_choice}\")\n",
    "\n",
    "        try:\n",
    "            instance_id = instance_data.get(\"id\", \"unknown\")\n",
    "            workflow_path = f\"Generated_{instance_id}_Workflow.drawio\"\n",
    "\n",
    "            if pattern_choice == \"template\":\n",
    "                # Sicherstellen, dass pattern_structure geladen ist\n",
    "                if not pattern_structure:\n",
    "                    file_path = f\"Patterns/{pattern}-pattern.ttl\"\n",
    "                    if os.path.exists(file_path):\n",
    "                        print(f\"üìÇ Reloading pattern file: {file_path}\")\n",
    "                        pattern_structure = extract_pattern_structure_from_file(file_path)\n",
    "                    else:\n",
    "                        print(f\"‚ùå Pattern-file missing or unable to load: {file_path}\")\n",
    "                        return\n",
    "\n",
    "                generate_template_from_instance()\n",
    "                template_path = f\"Updated_{instance_id}_Workflow.xml\"\n",
    "                finalize_and_merge_xml(template_path)\n",
    "\n",
    "            elif pattern_choice in [\"generate\", \"upload\"]:\n",
    "                # ‚úÖ NEU: Falls generate gew√§hlt wurde, aber pattern_structure noch fehlt ‚Üí nachladen\n",
    "                if not pattern_structure and pattern not in (\"no pattern\", None):\n",
    "                    file_path = f\"Patterns/{pattern}-pattern.ttl\"\n",
    "                    if os.path.exists(file_path):\n",
    "                        print(f\"üìÇ Auto-loading pattern file: {file_path}\")\n",
    "                        pattern_structure = extract_pattern_structure_from_file(file_path)\n",
    "                    else:\n",
    "                        print(f\"‚ùå Pattern-file missing: {file_path}\")\n",
    "                        return\n",
    "\n",
    "                run_component_mapping()\n",
    "                apply_positioning_algorithm()\n",
    "\n",
    "                xml_output = generate_drawio_xml(positions, components, edges, label_map)\n",
    "\n",
    "                with open(workflow_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(xml_output)\n",
    "\n",
    "                finalize_and_merge_xml(workflow_path)\n",
    "\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No valid option selected\")\n",
    "\n",
    "        except Exception as e:\n",
    "            show_exception(e)\n",
    "\n",
    "    pattern_decision_continue_button.layout.display = \"none\"\n",
    "\n",
    "\n",
    "\n",
    "pattern_decision_continue_button.on_click(on_pattern_decision_continue)\n",
    "\n",
    "# üÜï Anzeige des Buttons und Ausgabecontainers\n",
    "display(pattern_decision_continue_button, download_output)\n",
    "\n",
    "# ‚úÖ Anzeigen im Notebook\n",
    "#display(component_mapping_output, edge_mapping_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
